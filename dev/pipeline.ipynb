{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pypdfium2 as pdfium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pdfium.PdfDocument(\"./books/02450_Book.pdf\")\n",
    "\n",
    "text_all = \"\"\n",
    "for page in pdf:\n",
    "    textpage = page.get_textpage()\n",
    "    text_all += \" \".join(textpage.get_text_range().splitlines())\n",
    "\n",
    "\n",
    "text_split = list(re.split('(\\. |\\? |\\! )', text_all))\n",
    "text_sentences = []\n",
    "i = 0\n",
    "n = len(text_split)\n",
    "while i < n-1:\n",
    "    text_sentences.append(text_split[i]+text_split[i+1][:-1])\n",
    "    i += 2\n",
    "if i == n-1:\n",
    "    text_sentences.append(text_split[i])\n",
    "text_sentences = [s for s in text_sentences if len(s) > 10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./neural_search/data/2450.pkl', 'rb') as f:\n",
    "    df_questions = pickle.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_ctx_short = df_questions['context'].unique()\n",
    "\n",
    "ctx_mapping_short_to_medium = {}\n",
    "\n",
    "for i in range(len(u_ctx_short)-1):\n",
    "    if i % 2 == 0:\n",
    "        ctx_mapping_short_to_medium[u_ctx_short[i]] = u_ctx_short[i]+u_ctx_short[i+1]\n",
    "    else:\n",
    "        ctx_mapping_short_to_medium[u_ctx_short[i]] = u_ctx_short[i-1]+u_ctx_short[i]\n",
    "\n",
    "\n",
    "data = []\n",
    "for ctx in df_questions['context']:\n",
    "    val = ctx_mapping_short_to_medium.get(ctx)\n",
    "    if val:\n",
    "        data.append(val)\n",
    "    else:\n",
    "        data.append(ctx)\n",
    "\n",
    "df_questions['context_medium'] = data\n",
    "\n",
    "u_ctx_medium = df_questions['context_medium'].unique()\n",
    "\n",
    "ctx_mapping_medium_to_long = {}\n",
    "\n",
    "n = len(u_ctx_medium)\n",
    "for i in range(n-1,0,-1):\n",
    "    if i % 2 == n % 2:\n",
    "        ctx_mapping_medium_to_long[u_ctx_medium[i]] = u_ctx_medium[i]+u_ctx_medium[i+1]\n",
    "    else:\n",
    "        ctx_mapping_medium_to_long[u_ctx_medium[i]] = u_ctx_medium[i-1]+u_ctx_medium[i]\n",
    "\n",
    "data = []\n",
    "for ctx in df_questions['context_medium']:\n",
    "    val = ctx_mapping_medium_to_long.get(ctx)\n",
    "    if val:\n",
    "        data.append(val)\n",
    "    else:\n",
    "        data.append(ctx)\n",
    "\n",
    "df_questions['context_long'] = data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating vector embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "embedding_model = SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-dot-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_embeddings = {q: embedding_model.encode(q) for q in df_questions['question'].unique()}\n",
    "context_embeddings = {ctx: embedding_model.encode(ctx) for ctx in df_questions['context'].unique()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./neural_search/data/2450_question_embeddings.pkl', 'wb') as f:\n",
    "    f.write(pickle.dumps(question_embeddings))\n",
    "with open('./neural_search/data/2450_context_embeddings_1000.pkl', 'wb') as f:\n",
    "    f.write(pickle.dumps(context_embeddings))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "is_added = defaultdict(lambda: 0)\n",
    "\n",
    "contexts = df_questions['context'].unique()\n",
    "data = []\n",
    "\n",
    "for i,row in df_questions.iterrows():\n",
    "    #Avoid duplicate data-points. Skip if question if it has already been matched with contexts\n",
    "    if is_added[row['question']]:\n",
    "        continue\n",
    "\n",
    "    dft = pd.DataFrame(columns=['context', 'question', 'label'])\n",
    "    dft['context'] = contexts\n",
    "    dft['question'] = row['question']\n",
    "    dft['label'] = 0\n",
    "\n",
    "    #Update label to 1 if question was generated from the context\n",
    "    for ctx in df_questions.loc[df_questions['question'] == row['question'],'context']:\n",
    "        dft.loc[dft['context']==ctx,'label'] = 1\n",
    "\n",
    "    data.append(dft)\n",
    "\n",
    "    is_added[row['question']] = 1\n",
    "df = pd.concat(data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test_q = int(0.25*len(df['question'].unique()))\n",
    "test_q = np.random.choice(df['question'].unique(), n_test_q, replace=False)\n",
    "\n",
    "df_test = df.loc[df['question'].isin(test_q)]\n",
    "df_test.reset_index(inplace=True, drop=True)\n",
    "df_train = df.loc[~df['question'].isin(test_q)]\n",
    "df_train.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting context question pairs to vector embeddings\n",
    "\n",
    "X_train = np.array([np.concatenate((context_embeddings[ctx],question_embeddings[q])) for ctx,q in zip(df_train['context'].values, df_train['question'].values)])\n",
    "y_train = np.array([i for i in df_train['label'].values])\n",
    "\n",
    "X_test = np.array([np.concatenate((context_embeddings[ctx],question_embeddings[q])) for ctx,q in zip(df_test['context'].values, df_test['question'].values)])\n",
    "y_test = np.array([i for i in df_test['label'].values])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training NS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "neural_net = Sequential()\n",
    "neural_net.add(Dense(768, input_dim=768*2, activation='relu'))\n",
    "neural_net.add(Dense(384, activation='relu'))\n",
    "neural_net.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "neural_net.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "class BatchBalancerSequence(tf.keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x_p = x_set[y_set==1]\n",
    "        self.x_n = x_set[y_set==0]\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_size_n = batch_size//2\n",
    "\n",
    "    def __len__(self):\n",
    "        #For each n y=0 we will oversample with n y=1\n",
    "        return int(np.ceil(len(self.x_n)*2 / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        low = idx * self.batch_size_n\n",
    "        # Cap upper bound at array length; the last batch may be smaller\n",
    "        # if the total number of items is not a multiple of batch size.\n",
    "        high = min(low + self.batch_size_n, len(self.x_n))\n",
    "        batch_x = self.x_n[low:high]\n",
    "        n_neg = len(batch_x)\n",
    "        batch_y = [0]*n_neg\n",
    "        \n",
    "        x_p_idx = np.random.choice(len(self.x_p), n_neg, replace=False)\n",
    "        batch_x = np.concatenate((batch_x, self.x_p[x_p_idx]))\n",
    "        batch_y = np.append(batch_y, [1]*n_neg)\n",
    "\n",
    "        return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = neural_net.fit(BatchBalancerSequence(X_train, y_train, 128), epochs=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_roc(name, labels, predictions, **kwargs):\n",
    "  fp, tp, _ = roc_curve(labels, predictions)\n",
    "\n",
    "  plt.plot(100*fp, 100*tp, label=f'{name} AUC: {roc_auc_score(labels, predictions):.3f}', linewidth=2, **kwargs)\n",
    "  plt.xlabel('False positives [%]')\n",
    "  plt.ylabel('True positives [%]')\n",
    "  plt.xlim([-0.5,100])\n",
    "  plt.ylim([0,100.5])\n",
    "  plt.grid(True)\n",
    "  ax = plt.gca()\n",
    "  ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_nn_preds = neural_net.predict(X_test)\n",
    "y_cosine_preds = [np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b)) for a,b in zip(X_test[:,:1536//2], X_test[:,1536//2:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plot_roc(\"NS\", y_test, y_nn_preds)\n",
    "plot_roc(\"Cosine\", y_test, y_cosine_preds)\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./neural_search/data/y_2450_labels.pkl', 'wb') as f:\n",
    "    f.write(pickle.dumps(y_test))\n",
    "with open('./neural_search/data/y_2450_nn.pkl', 'wb') as f:\n",
    "    f.write(pickle.dumps(y_nn_preds))\n",
    "with open('./neural_search/data/y_2450_cos.pkl', 'wb') as f:\n",
    "    f.write(pickle.dumps(y_cosine_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
