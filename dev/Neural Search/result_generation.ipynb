{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "random.seed(23)\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-dot-v1')\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "with open('./data/history.pkl', 'rb') as f:\n",
    "    df_data = pickle.load(f)\n",
    "df_data.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_ctx_short = df_data['context'].unique()\n",
    "\n",
    "ctx_mapping_short_to_medium = {}\n",
    "\n",
    "for i in range(len(u_ctx_short)-1):\n",
    "    if i % 2 == 0:\n",
    "        ctx_mapping_short_to_medium[u_ctx_short[i]] = u_ctx_short[i]+u_ctx_short[i+1]\n",
    "    else:\n",
    "        ctx_mapping_short_to_medium[u_ctx_short[i]] = u_ctx_short[i-1]+u_ctx_short[i]\n",
    "\n",
    "data = []\n",
    "for ctx in df_data['context']:\n",
    "    val = ctx_mapping_short_to_medium.get(ctx)\n",
    "    if val:\n",
    "        data.append(val)\n",
    "    else:\n",
    "        data.append(ctx)\n",
    "\n",
    "df_data['context_medium'] = data\n",
    "\n",
    "u_ctx_medium = df_data['context_medium'].unique()\n",
    "\n",
    "ctx_mapping_medium_to_long = {}\n",
    "\n",
    "n = len(u_ctx_medium)\n",
    "for i in range(n-1,0,-1):\n",
    "    if i % 2 == n % 2:\n",
    "        ctx_mapping_medium_to_long[u_ctx_medium[i]] = u_ctx_medium[i]+u_ctx_medium[i+1]\n",
    "    else:\n",
    "        ctx_mapping_medium_to_long[u_ctx_medium[i]] = u_ctx_medium[i-1]+u_ctx_medium[i]\n",
    "\n",
    "data = []\n",
    "for ctx in df_data['context_medium']:\n",
    "    val = ctx_mapping_medium_to_long.get(ctx)\n",
    "    if val:\n",
    "        data.append(val)\n",
    "    else:\n",
    "        data.append(ctx)\n",
    "\n",
    "df_data['context_long'] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make embeddings\n",
    "question_embeddings = {q: model.encode(q) for q in df_data['question'].unique()}\n",
    "with open('./Embeddings/hist_question_embeddings.pkl', 'wb') as f:\n",
    "    f.write(pickle.dumps(question_embeddings))\n",
    "\n",
    "context_embeddings_1000 = {ctx: model.encode(ctx) for ctx in df_data['context'].unique()}\n",
    "with open('./Embeddings/hist_context_embeddings_1000.pkl', 'wb') as f:\n",
    "    f.write(pickle.dumps(context_embeddings_1000))\n",
    "\n",
    "context_embeddings_2000 = {ctx: model.encode(ctx) for ctx in df_data['context_medium'].unique()}\n",
    "with open('./Embeddings/hist_context_embeddings_2000.pkl', 'wb') as f:\n",
    "    f.write(pickle.dumps(context_embeddings_2000))\n",
    "\n",
    "context_embeddings_4000 = {ctx: model.encode(ctx) for ctx in df_data['context_long'].unique()}\n",
    "with open('./Embeddings/hist_context_embeddings_4000.pkl', 'wb') as f:\n",
    "    f.write(pickle.dumps(context_embeddings_4000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define embeddings - remember to change the embeddings file to 1000, 2000 or 4000\n",
    "# Select context, context_medium or context_long depending on 1000, 2000 or 4000\n",
    "embedding_length = 1000\n",
    "ctx_length_to_name = {1000: 'context', 2000: 'context_medium', 4000: 'context_long'}\n",
    "\n",
    "with open(f'./Embeddings/hist_context_embeddings_{embedding_length}.pkl', 'rb') as f:\n",
    "    context_embeddings = pickle.loads(f.read())\n",
    "with open('./Embeddings/hist_question_embeddings.pkl', 'rb') as f:\n",
    "    question_embeddings = pickle.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make combinations\n",
    "from collections import defaultdict\n",
    "\n",
    "is_added = defaultdict(lambda: 0)\n",
    "\n",
    "contexts = df_data[ctx_length_to_name[embedding_length]].unique()\n",
    "data = []\n",
    "\n",
    "for i,row in df_data.iterrows():\n",
    "    if is_added[row['question']]:\n",
    "        continue\n",
    "\n",
    "    dft = pd.DataFrame(columns=['context', 'question', 'label'])\n",
    "    dft['context'] = contexts\n",
    "    dft['question'] = row['question']\n",
    "    dft['label'] = 0\n",
    "\n",
    "    for ctx in df_data.loc[df_data['question'] == row['question'], ctx_length_to_name[embedding_length]]:\n",
    "        dft.loc[dft['context']==ctx,'label'] = 1\n",
    "\n",
    "    data.append(dft)\n",
    "\n",
    "    is_added[row['question']] = 1\n",
    "df = pd.concat(data, ignore_index=True)\n",
    "sum(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select for train and test\n",
    "n_test_q = int(0.2*len(df['question'].unique()))\n",
    "test_q = np.random.choice(df['question'].unique(), n_test_q, replace=False)\n",
    "\n",
    "# Make dataframes for repeated contexts\n",
    "df_test_all_ctx = df.loc[df['question'].isin(test_q)]\n",
    "df_test_all_ctx.reset_index(inplace=True, drop=True)\n",
    "df_par_all_ctx =  df.loc[~df['question'].isin(test_q)]\n",
    "df_par_all_ctx.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Select validation questions\n",
    "n_val_q = int(0.1*len(df_par_all_ctx['question'].unique()))\n",
    "val_q = np.random.choice(df_par_all_ctx['question'].unique(), n_val_q, replace=False)\n",
    "\n",
    "# Make dataframes for train and validation\n",
    "df_val_all_ctx =  df_par_all_ctx.loc[df_par_all_ctx['question'].isin(val_q)]\n",
    "df_val_all_ctx.reset_index(inplace=True, drop=True)\n",
    "df_train_all_ctx =  df_par_all_ctx.loc[~df_par_all_ctx['question'].isin(val_q)]\n",
    "df_train_all_ctx.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Make train and test data\n",
    "X_train_all_ctx = np.array([np.concatenate((context_embeddings[ctx],question_embeddings[q])) for ctx,q in zip(df_train_all_ctx['context'].values, df_train_all_ctx['question'].values)])\n",
    "y_train_all_ctx = np.array([i for i in df_train_all_ctx['label'].values])\n",
    "\n",
    "X_val_all_ctx = np.array([np.concatenate((context_embeddings[ctx],question_embeddings[q])) for ctx,q in zip(df_val_all_ctx['context'].values, df_val_all_ctx['question'].values)])\n",
    "y_val_all_ctx = np.array([i for i in df_val_all_ctx['label'].values])\n",
    "\n",
    "X_test_all_ctx = np.array([np.concatenate((context_embeddings[ctx],question_embeddings[q])) for ctx,q in zip(df_test_all_ctx['context'].values, df_test_all_ctx['question'].values)])\n",
    "y_test_all_ctx = np.array([i for i in df_test_all_ctx['label'].values])\n",
    "\n",
    "# Shuffle\n",
    "idx_train_all_ctx = np.random.permutation(len(X_train_all_ctx))\n",
    "X_train_all_ctx = X_train_all_ctx[idx_train_all_ctx]\n",
    "y_train_all_ctx = y_train_all_ctx[idx_train_all_ctx]\n",
    "\n",
    "idx_val_all_ctx = np.random.permutation(len(X_val_all_ctx))\n",
    "X_val_all_ctx = X_val_all_ctx[idx_val_all_ctx]\n",
    "y_val_all_ctx = y_val_all_ctx[idx_val_all_ctx]\n",
    "\n",
    "idx_test_all_ctx = np.random.permutation(len(X_test_all_ctx))\n",
    "X_test_all_ctx = X_test_all_ctx[idx_test_all_ctx]\n",
    "y_test_all_ctx = y_test_all_ctx[idx_test_all_ctx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data\n",
    "with open(f'./Results/{embedding_length}/y_labels', 'wb') as f:\n",
    "    f.write(pickle.dumps(y_test_all_ctx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define NN\n",
    "neural_net_b = Sequential()\n",
    "neural_net_b.add(Dense(768, input_dim=768*2, activation='relu'))\n",
    "neural_net_b.add(Dense(384, activation='relu'))\n",
    "neural_net_b.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "neural_net_b.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', 'loss', 'prc', 'precision', 'recall'])\n",
    "\n",
    "# Train NN\n",
    "history_basic = neural_net_b.fit(X_train_all_ctx, y_train_all_ctx, batch_size=128, epochs=10, validation_data=(X_val_all_ctx, y_val_all_ctx))\n",
    "neural_net_b.save(f'./Results/{embedding_length}/hist_nn')\n",
    "\n",
    "# Test NN\n",
    "y_nnb_preds = neural_net_b.predict(X_test_all_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "with open(f'./Results/{embedding_length}/nn_preds', 'wb') as f:\n",
    "    f.write(pickle.dumps(y_nnb_preds))\n",
    "\n",
    "with open(f'./Results/{embedding_length}/nn_hist', 'wb') as f:\n",
    "    f.write(pickle.dumps(history_basic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(history):\n",
    "  metrics = ['loss', 'prc', 'precision', 'recall']\n",
    "  for n, metric in enumerate(metrics):\n",
    "    name = metric.replace(\"_\",\" \").capitalize()\n",
    "    plt.subplot(2,2,n+1)\n",
    "    plt.plot(history.epoch, history.history[metric], label='Train')\n",
    "    plt.plot(history.epoch, history.history['val_'+metric],\n",
    "              linestyle=\"--\", label='Val')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(name)\n",
    "    if metric == 'loss':\n",
    "      plt.ylim([0, plt.ylim()[1]])\n",
    "    elif metric == 'auc':\n",
    "      plt.ylim([0.8,1])\n",
    "    else:\n",
    "      plt.ylim([0,1])\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(history_basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted dot-product\n",
    "learning_rate = 0.1\n",
    "loss_list = []\n",
    "val_list = []\n",
    "\n",
    "T = tf.Variable(np.ones(X_train_all_ctx.shape[1]//2))\n",
    "\n",
    "len_train = X_train_all_ctx.shape[0]\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "batch_size = 128\n",
    "itt = int(np.ceil(len_train/batch_size))\n",
    "\n",
    "n_epochs = 10\n",
    "\n",
    "bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "with tqdm(total=itt*n_epochs) as pbar:\n",
    "    for _ in range(n_epochs):\n",
    "        for i in range(1,itt+1):\n",
    "            low = (i-1)*batch_size\n",
    "            high = min(i*batch_size, len_train)\n",
    "\n",
    "            a = X_train_all_ctx[low:high,:1536//2]\n",
    "            b = X_train_all_ctx[low:high,1536//2:]\n",
    "\n",
    "            y_true = y_train_all_ctx[low:high]\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                tape.watch(T)  # Watch the variable T for gradient computation\n",
    "                W = tf.exp(T)\n",
    "                y_preds = tf.math.reduce_sum((a*W*b), axis=1)\n",
    "                error = bce(y_true, y_preds)\n",
    "\n",
    "            gradients = tape.gradient(error, T)\n",
    "            optimizer.apply_gradients([(gradients, T)])\n",
    "            loss_list.append(error.numpy())\n",
    "            pbar.update(1)\n",
    "    \n",
    "        W_val = tf.exp(T).numpy()\n",
    "        y_val_preds = [tf.math.reduce_sum((a*W_val*b), axis=1) for a,b in zip(X_val_all_ctx[:,:1536//2], X_val_all_ctx[:,1536//2:])]\n",
    "        val_list.append(bce(y_val_all_ctx, y_val_preds).numpy())\n",
    "\n",
    "loss_list_epochs = []\n",
    "for i in range(n_epochs):\n",
    "    loss_list_epochs.append(np.mean(loss_list[i*itt:(i+1)*itt]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weight information\n",
    "with open(f'./Results/{embedding_length}/dot_weights', 'wb') as f:\n",
    "    f.write(pickle.dumps(T))\n",
    "\n",
    "with open(f'./Results/{embedding_length}/weight_loss', 'wb') as f:\n",
    "    f.write(pickle.dumps(loss_list_epochs))\n",
    "\n",
    "with open(f'./Results/{embedding_length}/weight_val', 'wb') as f:\n",
    "    f.write(pickle.dumps(val_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight predictions\n",
    "W = tf.exp(T).numpy()\n",
    "y_weight_preds = [tf.math.reduce_sum((a*W*b), axis=1) for a,b in zip(X_test_all_ctx[:,:1536//2], X_test_all_ctx[:,1536//2:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dot predictions\n",
    "y_dot_preds = [np.dot(a, b) for a,b in zip(X_test_all_ctx[:,:1536//2], X_test_all_ctx[:,1536//2:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(name, labels, predictions, **kwargs):\n",
    "  fp, tp, _ = roc_curve(labels, predictions)\n",
    "  plt.plot(fp, tp, label=f'{name} AUC: {roc_auc_score(labels, predictions):.3f}', linewidth=2, **kwargs)\n",
    "  plt.xlabel('False positives [%]')\n",
    "  plt.ylabel('True positives [%]')\n",
    "  plt.xlim([-0.05,1])\n",
    "  plt.ylim([0,1.05])\n",
    "  plt.grid(True)\n",
    "  ax = plt.gca()\n",
    "  ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve\n",
    "plt.figure(figsize=(10,6))\n",
    "plot_roc(\"NS\", y_test_all_ctx, y_nnb_preds)\n",
    "plot_roc(\"Weight\", y_test_all_ctx, y_weight_preds)\n",
    "plot_roc(\"Dot\", y_test_all_ctx, y_dot_preds)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, label='Random guess')\n",
    "plt.title(\"ROC curve for Neural Search\")\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig(f'./Results/{embedding_length}/search_{embedding_length}.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
