{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing context extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book: ww2, Embedding length: 4000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define embeddings - remember to change the embeddings file to 1000, 2000 or 4000\n",
    "# Select context, context_medium or context_long depending on 1000, 2000 or 4000\n",
    "book = [2450, 'ww2'][1]\n",
    "embedding_length = [1000, 2000, 4000][2]\n",
    "print(f\"book: {book}, Embedding length: {embedding_length}\")\n",
    "ctx_length_to_name = {1000: 'context', 2000: 'context_medium', 4000: 'context_long'}\n",
    "\n",
    "# use cuda\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get data\n",
    "with open(f'Neural Search/data/{book}.pkl', 'rb') as f:\n",
    "    df_data = pickle.load(f)\n",
    "df_data.reset_index(inplace=True)\n",
    "with open(f'Neural Search/Embeddings/{book}_context_embeddings_{embedding_length}.pkl', 'rb') as f:\n",
    "    context_embeddings = pickle.loads(f.read())\n",
    "with open(f'Neural Search/Embeddings/{book}_question_embeddings.pkl', 'rb') as f:\n",
    "    question_embeddings = pickle.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_ctx_short = df_data['context'].unique()\n",
    "\n",
    "ctx_mapping_short_to_medium = {}\n",
    "\n",
    "for i in range(len(u_ctx_short)-1):\n",
    "    if i % 2 == 0:\n",
    "        ctx_mapping_short_to_medium[u_ctx_short[i]] = u_ctx_short[i]+u_ctx_short[i+1]\n",
    "    else:\n",
    "        ctx_mapping_short_to_medium[u_ctx_short[i]] = u_ctx_short[i-1]+u_ctx_short[i]\n",
    "\n",
    "data = []\n",
    "for ctx in df_data['context']:\n",
    "    val = ctx_mapping_short_to_medium.get(ctx)\n",
    "    if val:\n",
    "        data.append(val)\n",
    "    else:\n",
    "        data.append(ctx)\n",
    "\n",
    "df_data['context_medium'] = data\n",
    "\n",
    "u_ctx_medium = df_data['context_medium'].unique()\n",
    "\n",
    "ctx_mapping_medium_to_long = {}\n",
    "\n",
    "n = len(u_ctx_medium)\n",
    "for i in range(n-1,0,-1):\n",
    "    if i % 2 == n % 2:\n",
    "        ctx_mapping_medium_to_long[u_ctx_medium[i]] = u_ctx_medium[i]+u_ctx_medium[i+1]\n",
    "    else:\n",
    "        ctx_mapping_medium_to_long[u_ctx_medium[i]] = u_ctx_medium[i-1]+u_ctx_medium[i]\n",
    "\n",
    "data = []\n",
    "for ctx in df_data['context_medium']:\n",
    "    val = ctx_mapping_medium_to_long.get(ctx)\n",
    "    if val:\n",
    "        data.append(val)\n",
    "    else:\n",
    "        data.append(ctx)\n",
    "\n",
    "df_data['context_long'] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1255"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make combinations\n",
    "from collections import defaultdict\n",
    "\n",
    "is_added = defaultdict(lambda: 0)\n",
    "\n",
    "contexts = df_data[ctx_length_to_name[embedding_length]].unique()\n",
    "data = []\n",
    "\n",
    "for i,row in df_data.iterrows():\n",
    "    if is_added[row['question']]:\n",
    "        continue\n",
    "\n",
    "    dft = pd.DataFrame(columns=['context', 'question', 'label'])\n",
    "    dft['context'] = contexts\n",
    "    dft['question'] = row['question']\n",
    "    dft['label'] = 0\n",
    "\n",
    "    for ctx in df_data.loc[df_data['question'] == row['question'], ctx_length_to_name[embedding_length]]:\n",
    "        dft.loc[dft['context']==ctx,'label'] = 1\n",
    "\n",
    "    data.append(dft)\n",
    "\n",
    "    is_added[row['question']] = 1\n",
    "df = pd.concat(data, ignore_index=True)\n",
    "sum(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select for train and test\n",
    "n_test_q = int(0.2*len(df['question'].unique()))\n",
    "test_q = np.random.choice(df['question'].unique(), n_test_q, replace=False)\n",
    "\n",
    "# # Make dataframes for repeated contexts\n",
    "# df_test_all_ctx = df.loc[df['question'].isin(test_q)]\n",
    "# df_test_all_ctx.reset_index(inplace=True, drop=True)\n",
    "df_par_all_ctx =  df.loc[~df['question'].isin(test_q)]\n",
    "df_par_all_ctx.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# # Select validation questions\n",
    "n_val_q = int(0.1*len(df_par_all_ctx['question'].unique()))\n",
    "val_q = np.random.choice(df_par_all_ctx['question'].unique(), n_val_q, replace=False)\n",
    "\n",
    "# # Make dataframes for train and validation\n",
    "# df_val_all_ctx =  df_par_all_ctx.loc[df_par_all_ctx['question'].isin(val_q)]\n",
    "# df_val_all_ctx.reset_index(inplace=True, drop=True)\n",
    "df_train_all_ctx =  df_par_all_ctx.loc[~df_par_all_ctx['question'].isin(val_q)]\n",
    "df_train_all_ctx.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Make train and test data - Context and question are concatenated together\n",
    "X_train_all_ctx = np.array([np.concatenate((context_embeddings[ctx],question_embeddings[q])) for ctx,q in zip(df_train_all_ctx['context'].values, df_train_all_ctx['question'].values)])\n",
    "y_train_all_ctx = np.array([i for i in df_train_all_ctx['label'].values])\n",
    "\n",
    "# X_val_all_ctx = np.array([np.concatenate((context_embeddings[ctx],question_embeddings[q])) for ctx,q in zip(df_val_all_ctx['context'].values, df_val_all_ctx['question'].values)])\n",
    "# y_val_all_ctx = np.array([i for i in df_val_all_ctx['label'].values])\n",
    "\n",
    "# X_test_all_ctx = np.array([np.concatenate((context_embeddings[ctx],question_embeddings[q])) for ctx,q in zip(df_test_all_ctx['context'].values, df_test_all_ctx['question'].values)])\n",
    "# y_test_all_ctx = np.array([i for i in df_test_all_ctx['label'].values])\n",
    "\n",
    "# # Shuffle\n",
    "# idx_train_all_ctx = np.random.permutation(len(X_train_all_ctx))\n",
    "# X_train_all_ctx = X_train_all_ctx[idx_train_all_ctx]\n",
    "# y_train_all_ctx = y_train_all_ctx[idx_train_all_ctx]\n",
    "\n",
    "# idx_val_all_ctx = np.random.permutation(len(X_val_all_ctx))\n",
    "# X_val_all_ctx = X_val_all_ctx[idx_val_all_ctx]\n",
    "# y_val_all_ctx = y_val_all_ctx[idx_val_all_ctx]\n",
    "\n",
    "# idx_test_all_ctx = np.random.permutation(len(X_test_all_ctx))\n",
    "# X_test_all_ctx = X_test_all_ctx[idx_test_all_ctx]\n",
    "# y_test_all_ctx = y_test_all_ctx[idx_test_all_ctx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def show_book_contexts(values, title=\"\", size=(14, 1), linewidth=1):\n",
    "    length = len(values)\n",
    "    values = (values - values.min())/(values.max() - values.min())\n",
    "\n",
    "    # fig, ax = plt.subplots()\n",
    "    fig, ax = plt.subplots(figsize=size)\n",
    "    _, _, patches = ax.hist(range(length), length, edgecolor='black', linewidth=linewidth)\n",
    "    for i, v in enumerate(values):\n",
    "        patches[i].set_facecolor(color=colorFader('#1f77b4', 'red', v))\n",
    "    # for i, v in enumerate(values):\n",
    "    #     ax.axvline(i, color=colorFader('#1f77b4', 'red', v), linewidth=1) \n",
    "    # plt.plot(values)\n",
    "    # values = np.array(values).flatten()\n",
    "    # plt.bar(range(len(values)), values)\n",
    "    if title:\n",
    "        plt.title(title, fontsize=15)\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def colorFader(c1,c2,mix=0): #fade (linear interpolate) from color c1 (at mix=0) to c2 (mix=1)\n",
    "    c1=np.array(mpl.colors.to_rgb(c1))\n",
    "    c2=np.array(mpl.colors.to_rgb(c2))\n",
    "    return mpl.colors.to_hex((1-mix)*c1 + mix*c2)\n",
    "\n",
    "# show_book_contexts(output, \"\", (6,0.7), 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(query) = 768\n",
      "contexts.shape = (64, 768)\n",
      "query_book.shape = (64, 1536)\n",
      "2/2 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeQAAABJCAYAAADsW1V9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAACz0lEQVR4nO3cu24TURSG0e1L5yZpokQ2iIY3yHNwKaipKIA3AQoaGgou75OeCgGSLUWRSJrQYQ8FAjK5AArG88tZq7LO0fHsmSKfJorSa5qmKQCgU/2uBwAABBkAIggyAAQQZAAIIMgAEECQASCAIANAAEEGgACCDAABBBkAAgy7uvDu7m7t7++fu3dwcFDz+bwGg0FtbW39cf2ye6s6YwYzrPsM63hPZriaM/ywvb1de3t75+79L72u/pf1ZDKp2WzWxaUB4LfG43FNp9OVXrOzN+Sfev0ajDZbS/Pjz+fuXbR+2b1VnTGDGdZ9hnW8p78506+qnWr7+ZpxhZ7DKmcYjjZaZ74eHy53hi9HVc2iutB5kAejzZo8ftta+/TkVlWzOLN30frpveuPXrf2Pjy980/f969nlv19ZjBD2gzreE8n1288fNU68/7Z3apmUTtVdfodalBVizr7s20dnsOyZ7j2+E3rzMcnt0/svT2x/v3McLRRNx+8bJ159/xeVdMs7Z6mL+7/CvaK+aMuAAggyAAQQJABIIAgA0AAQQaAAIIMAAEEGQACCDIABBBkAAggyAAQQJABIIAgA0AAQQaAAIIMAAEEGQACCDIABBBkAAggyAAQQJABIIAgA0AAQQaAAIIMAAEEGQACCDIABBBkAAggyAAQQJABIIAgA0AAQQaAAIIMAAEEGQACCDIABBBkAAggyAAQQJABIIAgA0AAQQaAAIIMAAEEGQACCDIABBBkAAggyAAQQJABIIAgA0AAQQaAAIIMAAEEGQAC9Jqmabq48GQyqdlsVtXr12C02dqbH3/+/uHU3kXrl91b1RkzmGHdZ1jHe/qbM/2q2qm22Y8PV+g5rHKG4Wijdebr8eFyZ/hyVNUsajwe13Q6rVXqLMgAwC9+ZQ0AAQQZAAIIMgAEEGQACCDIABBAkAEggCADQABBBoAAggwAAb4BPCdaAs8aT5oAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 600x70 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sample a random question from the training data\n",
    "random_idx = np.random.randint(0, len(X_train_all_ctx))\n",
    "random_question = X_train_all_ctx[random_idx]\n",
    "\n",
    "query = random_question[:len(random_question)//2]\n",
    "contexts = np.array(list(context_embeddings.values()))\n",
    "print(f'{len(query) = }\\n{contexts.shape = }')\n",
    "\n",
    "query_book = np.array([np.concatenate((ctx, query)) for ctx in contexts])\n",
    "print(f'{query_book.shape = }')\n",
    "\n",
    "# Load model\n",
    "\n",
    "neural_net_b = load_model(f'Neural Search/Results/{embedding_length}/hist_nn_{book}')\n",
    "output = neural_net_b.predict(query_book)\n",
    "\n",
    "show_book_contexts(output, \"\", (6,0.7), 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
