{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UKPLab / sentence-transformers\n",
    "https://github.com/UKPLab/sentence-transformers/tree/master/examples/unsupervised_learning/query_generation\n",
    "\n",
    "- 1_programming_query_generation.py - We generate queries for all paragraphs from these articles \n",
    "- 2_programming_train_bi-encoder.py - We train a SentenceTransformer bi-encoder with these generated queries. This results in a model we can then use for sematic search (for the given Wikipedia articles).\n",
    "- 3_programming_semantic_search.py - Shows how the trained model can be used for semantic search\n",
    "\n",
    "## 1_programming_query_generation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In this example we train a semantic search model to search through Wikipedia\n",
    "articles about programming articles & technologies.\n",
    "\n",
    "We use the text paragraphs from the following Wikipedia articles:\n",
    "Assembly language, C , C Sharp , C++, Go , Java , JavaScript, Keras, Laravel, MATLAB, Matplotlib, MongoDB, MySQL, Natural Language Toolkit, NumPy, pandas (software), Perl, PHP, PostgreSQL, Python , PyTorch, R , React, Rust , Scala , scikit-learn, SciPy, Swift , TensorFlow, Vue.js\n",
    "\n",
    "In:\n",
    "1_programming_query_generation.py - We generate queries for all paragraphs from these articles\n",
    "2_programming_train_bi-encoder.py - We train a SentenceTransformer bi-encoder with these generated queries. This results in a model we can then use for sematic search (for the given Wikipedia articles).\n",
    "3_programming_semantic_search.py - Shows how the trained model can be used for semantic search\n",
    "\"\"\"\n",
    "import json\n",
    "import gzip\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "import tqdm\n",
    "import os\n",
    "from sentence_transformers import util\n",
    "\n",
    "paragraphs = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraphs: 1230\n"
     ]
    }
   ],
   "source": [
    "# We use the Wikipedia articles of certain programming languages\n",
    "corpus_filepath = 'wiki-programmming-20210101.jsonl.gz'\n",
    "if not os.path.exists(corpus_filepath):\n",
    "    util.http_get('https://sbert.net/datasets/wiki-programmming-20210101.jsonl.gz', corpus_filepath)\n",
    "\n",
    "with gzip.open(corpus_filepath, 'rt') as fIn:\n",
    "    for line in fIn:\n",
    "        data = json.loads(line.strip())\n",
    "\n",
    "        for p in data['paragraphs']:\n",
    "            if len(p) > 100:    #Only take paragraphs with at least 100 chars\n",
    "                paragraphs.add(p)\n",
    "\n",
    "paragraphs = list(paragraphs)\n",
    "print(\"Paragraphs:\", len(paragraphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "376.03333333333336"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# paragraphs[:5]\n",
    "# get lengths of first 10 paragraphs\n",
    "# len(paragraphs)\n",
    "# [len(p) for p in paragraphs]\n",
    "sum([len(p) for p in paragraphs]) / len(paragraphs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we load the model that is able to generate queries given a paragraph.\n",
    "# This model was trained on the MS MARCO dataset, a dataset with 500k\n",
    "# queries from Bing and the respective relevant passage\n",
    "tokenizer = T5Tokenizer.from_pretrained('BeIR/query-gen-msmarco-t5-large-v1')\n",
    "model = T5ForConditionalGeneration.from_pretrained('BeIR/query-gen-msmarco-t5-large-v1')\n",
    "model.eval()\n",
    "\n",
    "#Select the device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "# Parameters for generation\n",
    "batch_size = 8 #Batch size\n",
    "num_queries = 5 #Number of queries to generate for every paragraph\n",
    "max_length_paragraph = 300 #Max length for paragraph\n",
    "max_length_query = 64   #Max length for output query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]c:\\Users\\Jason\\miniconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3712: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)\n",
      "\n",
      "If you either need to use different keyword arguments for the source and target texts, you should do two calls like\n",
      "this:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "labels = tokenizer(text_target=tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n",
      "  0%|          | 0/10 [00:28<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m sub_paragraphs \u001b[39m=\u001b[39m paragraphs[start_idx:start_idx\u001b[39m+\u001b[39mbatch_size]\n\u001b[0;32m      6\u001b[0m inputs \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mprepare_seq2seq_batch(sub_paragraphs, max_length\u001b[39m=\u001b[39mmax_length_paragraph, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m----> 7\u001b[0m outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mgenerate(\n\u001b[0;32m      8\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs,\n\u001b[0;32m      9\u001b[0m     max_length\u001b[39m=\u001b[39mmax_length_query,\n\u001b[0;32m     10\u001b[0m     do_sample\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     11\u001b[0m     top_p\u001b[39m=\u001b[39m\u001b[39m0.95\u001b[39m,\n\u001b[0;32m     12\u001b[0m     num_return_sequences\u001b[39m=\u001b[39mnum_queries)\n\u001b[0;32m     14\u001b[0m \u001b[39mfor\u001b[39;00m idx, out \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(outputs):\n\u001b[0;32m     15\u001b[0m     query \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode(out, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Jason\\miniconda3\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Jason\\miniconda3\\lib\\site-packages\\transformers\\generation\\utils.py:1452\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, **kwargs)\u001b[0m\n\u001b[0;32m   1444\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   1445\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1446\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   1447\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   1448\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1449\u001b[0m     )\n\u001b[0;32m   1451\u001b[0m     \u001b[39m# 13. run sample\u001b[39;00m\n\u001b[1;32m-> 1452\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample(\n\u001b[0;32m   1453\u001b[0m         input_ids,\n\u001b[0;32m   1454\u001b[0m         logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[0;32m   1455\u001b[0m         logits_warper\u001b[39m=\u001b[39mlogits_warper,\n\u001b[0;32m   1456\u001b[0m         stopping_criteria\u001b[39m=\u001b[39mstopping_criteria,\n\u001b[0;32m   1457\u001b[0m         pad_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mpad_token_id,\n\u001b[0;32m   1458\u001b[0m         eos_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39meos_token_id,\n\u001b[0;32m   1459\u001b[0m         output_scores\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39moutput_scores,\n\u001b[0;32m   1460\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mreturn_dict_in_generate,\n\u001b[0;32m   1461\u001b[0m         synced_gpus\u001b[39m=\u001b[39msynced_gpus,\n\u001b[0;32m   1462\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1463\u001b[0m     )\n\u001b[0;32m   1465\u001b[0m \u001b[39melif\u001b[39;00m is_beam_gen_mode:\n\u001b[0;32m   1466\u001b[0m     \u001b[39mif\u001b[39;00m generation_config\u001b[39m.\u001b[39mnum_return_sequences \u001b[39m>\u001b[39m generation_config\u001b[39m.\u001b[39mnum_beams:\n",
      "File \u001b[1;32mc:\\Users\\Jason\\miniconda3\\lib\\site-packages\\transformers\\generation\\utils.py:2468\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   2465\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2467\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[1;32m-> 2468\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(\n\u001b[0;32m   2469\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs,\n\u001b[0;32m   2470\u001b[0m     return_dict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   2471\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m   2472\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   2473\u001b[0m )\n\u001b[0;32m   2475\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   2476\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jason\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Jason\\miniconda3\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1704\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1701\u001b[0m         decoder_attention_mask \u001b[39m=\u001b[39m decoder_attention_mask\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39mfirst_device)\n\u001b[0;32m   1703\u001b[0m \u001b[39m# Decode\u001b[39;00m\n\u001b[1;32m-> 1704\u001b[0m decoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(\n\u001b[0;32m   1705\u001b[0m     input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[0;32m   1706\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[0;32m   1707\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[0;32m   1708\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1709\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[0;32m   1710\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1711\u001b[0m     head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[0;32m   1712\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[0;32m   1713\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1714\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1715\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1716\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1717\u001b[0m )\n\u001b[0;32m   1719\u001b[0m sequence_output \u001b[39m=\u001b[39m decoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1721\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jason\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Jason\\miniconda3\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1074\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1061\u001b[0m     layer_outputs \u001b[39m=\u001b[39m checkpoint(\n\u001b[0;32m   1062\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m   1063\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1071\u001b[0m         \u001b[39mNone\u001b[39;00m,  \u001b[39m# past_key_value is always None with gradient checkpointing\u001b[39;00m\n\u001b[0;32m   1072\u001b[0m     )\n\u001b[0;32m   1073\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1074\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m   1075\u001b[0m         hidden_states,\n\u001b[0;32m   1076\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m   1077\u001b[0m         position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[0;32m   1078\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1079\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m   1080\u001b[0m         encoder_decoder_position_bias\u001b[39m=\u001b[39;49mencoder_decoder_position_bias,\n\u001b[0;32m   1081\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[0;32m   1082\u001b[0m         cross_attn_layer_head_mask\u001b[39m=\u001b[39;49mcross_attn_layer_head_mask,\n\u001b[0;32m   1083\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[0;32m   1084\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1085\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1086\u001b[0m     )\n\u001b[0;32m   1088\u001b[0m \u001b[39m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[0;32m   1089\u001b[0m \u001b[39m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[0;32m   1090\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Jason\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Jason\\miniconda3\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:719\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[1;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[0;32m    716\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    717\u001b[0m     query_length \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 719\u001b[0m cross_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer[\u001b[39m1\u001b[39;49m](\n\u001b[0;32m    720\u001b[0m     hidden_states,\n\u001b[0;32m    721\u001b[0m     key_value_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m    722\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[0;32m    723\u001b[0m     position_bias\u001b[39m=\u001b[39;49mencoder_decoder_position_bias,\n\u001b[0;32m    724\u001b[0m     layer_head_mask\u001b[39m=\u001b[39;49mcross_attn_layer_head_mask,\n\u001b[0;32m    725\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mcross_attn_past_key_value,\n\u001b[0;32m    726\u001b[0m     query_length\u001b[39m=\u001b[39;49mquery_length,\n\u001b[0;32m    727\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    728\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    729\u001b[0m )\n\u001b[0;32m    730\u001b[0m hidden_states \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    732\u001b[0m \u001b[39m# clamp inf values to enable fp16 training\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jason\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Jason\\miniconda3\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:634\u001b[0m, in \u001b[0;36mT5LayerCrossAttention.forward\u001b[1;34m(self, hidden_states, key_value_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, query_length, output_attentions)\u001b[0m\n\u001b[0;32m    621\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    622\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    623\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    631\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    632\u001b[0m ):\n\u001b[0;32m    633\u001b[0m     normed_hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(hidden_states)\n\u001b[1;32m--> 634\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mEncDecAttention(\n\u001b[0;32m    635\u001b[0m         normed_hidden_states,\n\u001b[0;32m    636\u001b[0m         mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    637\u001b[0m         key_value_states\u001b[39m=\u001b[39;49mkey_value_states,\n\u001b[0;32m    638\u001b[0m         position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[0;32m    639\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[0;32m    640\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[0;32m    641\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    642\u001b[0m         query_length\u001b[39m=\u001b[39;49mquery_length,\n\u001b[0;32m    643\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    644\u001b[0m     )\n\u001b[0;32m    645\u001b[0m     layer_output \u001b[39m=\u001b[39m hidden_states \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(attention_output[\u001b[39m0\u001b[39m])\n\u001b[0;32m    646\u001b[0m     outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m attention_output[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jason\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Jason\\miniconda3\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:530\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[1;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    525\u001b[0m value_states \u001b[39m=\u001b[39m project(\n\u001b[0;32m    526\u001b[0m     hidden_states, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv, key_value_states, past_key_value[\u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    527\u001b[0m )\n\u001b[0;32m    529\u001b[0m \u001b[39m# compute scores\u001b[39;00m\n\u001b[1;32m--> 530\u001b[0m scores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(\n\u001b[0;32m    531\u001b[0m     query_states, key_states\u001b[39m.\u001b[39;49mtranspose(\u001b[39m3\u001b[39;49m, \u001b[39m2\u001b[39;49m)\n\u001b[0;32m    532\u001b[0m )  \u001b[39m# equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9\u001b[39;00m\n\u001b[0;32m    534\u001b[0m \u001b[39mif\u001b[39;00m position_bias \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    535\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_relative_attention_bias:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Now for every paragraph in our corpus, we generate the queries\n",
    "with open('generated_queries.tsv', 'w') as fOut:\n",
    "    for start_idx in tqdm.trange(0, 10*batch_size, batch_size):\n",
    "    # for start_idx in tqdm.trange(0, len(paragraphs), batch_size):\n",
    "        sub_paragraphs = paragraphs[start_idx:start_idx+batch_size]\n",
    "        inputs = tokenizer.prepare_seq2seq_batch(sub_paragraphs, max_length=max_length_paragraph, truncation=True, return_tensors='pt').to(device)\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length_query,\n",
    "            do_sample=True,\n",
    "            top_p=0.95,\n",
    "            num_return_sequences=num_queries)\n",
    "\n",
    "        for idx, out in enumerate(outputs):\n",
    "            query = tokenizer.decode(out, skip_special_tokens=True)\n",
    "            para = sub_paragraphs[int(idx/num_queries)]\n",
    "            fOut.write(\"{}\\t{}\\n\".format(query.replace(\"\\t\", \" \").strip(), para.replace(\"\\t\", \" \").strip()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2_programming_train_bi-encoder.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses, models, datasets\n",
    "import os\n",
    "\n",
    "train_examples = []\n",
    "with open('generated_queries.tsv') as fIn:\n",
    "    for line in fIn:\n",
    "        query, paragraph = line.strip().split('\\t', maxsplit=1)\n",
    "        print(f'{query = }\\t{paragraph = }')\n",
    "        train_examples.append(InputExample(texts=[query, paragraph]))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62aa596b43c4436aba1b561e38be9a36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jason\\miniconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Jason\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5e50fa6f80848adab2de1636e7068c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "add4fc3df24b4da993962a0c9282de70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e5f7af340094d6bbbe4e69201620033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "007b3141b32c474e8eea2579cbaff869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For the MultipleNegativesRankingLoss, it is important\n",
    "# that the batch does not contain duplicate entries, i.e.\n",
    "# no two equal queries and no two equal paragraphs.\n",
    "# To ensure this, we use a special data loader\n",
    "train_dataloader = datasets.NoDuplicatesDataLoader(train_examples, batch_size=64)\n",
    "\n",
    "# Now we create a SentenceTransformer model from scratch\n",
    "word_emb = models.Transformer('distilbert-base-uncased')\n",
    "pooling = models.Pooling(word_emb.get_word_embedding_dimension())\n",
    "model = SentenceTransformer(modules=[word_emb, pooling])\n",
    "\n",
    "# MultipleNegativesRankingLoss requires input pairs (query, relevant_passage)\n",
    "# and trains the model so that is is suitable for semantic search\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9323f409f074decab420327cd07822f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca1231eefc4041418de005c8de31cc5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4449438b780f44f8b95edfbf9071eaf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ac64373931e40979401be7e5b9101e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tune the model\n",
    "num_epochs = 3\n",
    "warmup_steps = int(len(train_dataloader) * num_epochs * 0.1)\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=num_epochs, warmup_steps=warmup_steps, show_progress_bar=True)\n",
    "\n",
    "os.makedirs('output', exist_ok=True)\n",
    "model.save('output/programming-model')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3_programming_semantic_search.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import gzip\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Load the model we trained in 2_programming_train_bi-encoder.py\n",
    "model = SentenceTransformer('output/programming-model')\n",
    "\n",
    "# Load the corpus\n",
    "docs = []\n",
    "corpus_filepath = 'wiki-programmming-20210101.jsonl.gz'\n",
    "if not os.path.exists(corpus_filepath):\n",
    "    util.http_get('https://sbert.net/datasets/wiki-programmming-20210101.jsonl.gz', corpus_filepath)\n",
    "\n",
    "with gzip.open(corpus_filepath, 'rt') as fIn:\n",
    "    for line in fIn:\n",
    "        data = json.loads(line.strip())\n",
    "        title = data['title']\n",
    "        for p in data['paragraphs']:\n",
    "            if len(p) > 100:    #Only take paragraphs with at least 100 chars\n",
    "                docs.append((title, p))\n",
    "\n",
    "paragraph_emb = model.encode([d[1] for d in docs], convert_to_tensor=True)  # TODO: Should be done once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save paragraph embeddings to disk\n",
    "import pickle\n",
    "with open('output/programming-paragraph-embeddings.pkl', 'wb') as fOut:\n",
    "    pickle.dump(paragraph_emb, fOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1230, 768])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load paragraph embeddings from disk\n",
    "with open('output/programming-paragraph-embeddings.pkl', 'rb') as fIn:\n",
    "    _paragraph_emb = pickle.load(fIn)\n",
    "_paragraph_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Wikipedia Articles:\n",
      "Assembly language, C (programming language), C Sharp (programming language), C++, Go (programming language), Java (programming language), JavaScript, Keras, Laravel, MATLAB, Matplotlib, MongoDB, MySQL, Natural Language Toolkit, NumPy, PHP, Pandas (software), Perl, PostgreSQL, PyTorch, Python (programming language), R (programming language), Rust (programming language), Scala (programming language), SciPy, Scikit-learn, Swift (programming language), TensorFlow, Vue.js\n",
      "0.69\tPython (programming language)\t\tThe major academic conference on Python is PyCon. There are also special Python mentoring programmes, such as Pyladies.\n",
      "0.65\tPython (programming language)\t\tUsers and admirers of Python, especially those considered knowledgeable or experienced, are often referred to as \"Pythonistas\".\n",
      "0.64\tPython (programming language)\t\tPython's name is derived from the British comedy group Monty Python, whom Python creator Guido van Rossum enjoyed while developing the language. Monty Python references appear frequently in Python code and culture; for example, the metasyntactic variables often used in Python literature are \"spam\" and \"eggs\" instead of the traditional \"foo\" and \"bar\". The official Python documentation also contains various references to Monty Python routines.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Available Wikipedia Articles:\")\n",
    "print(\", \".join(sorted(list(set([d[0] for d in docs])))))\n",
    "\n",
    "query = \"What is Python?\"\n",
    "query_emb = model.encode(query, convert_to_tensor=True)\n",
    "hits = util.semantic_search(query_emb, paragraph_emb, top_k=3)[0]\n",
    "\n",
    "for hit in hits:\n",
    "    doc = docs[hit['corpus_id']]\n",
    "    print(\"{:.2f}\\t{}\\t\\t{}\".format(hit['score'], doc[0], doc[1]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sections pickle\n",
    "import pickle\n",
    "# add df_pickle to path\n",
    "with open('df_context.pkl', 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "\n",
    "# Extract context\n",
    "contexts = df['context'].tolist()\n",
    "contexts = [c.strip() for c in contexts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1183\n",
      "691\n",
      "838\n",
      "415\n",
      "1333\n",
      "3658\n",
      "296\n",
      "4139\n",
      "7414\n",
      "3579\n",
      "2250\n",
      "1411\n",
      "7674\n",
      "2930\n",
      "3979\n",
      "1654\n",
      "3990\n",
      "2898\n",
      "1766\n",
      "821\n",
      "2872\n",
      "2991\n",
      "9969\n",
      "1550\n",
      "1107\n",
      "2769\n",
      "5361\n",
      "3734\n",
      "4172\n",
      "1300\n",
      "10140\n",
      "1594\n",
      "1726\n",
      "3380\n",
      "3675\n",
      "2775\n",
      "8407\n",
      "7129\n",
      "3008\n",
      "3124\n",
      "1600\n",
      "1132\n",
      "1362\n",
      "4432\n",
      "1160\n",
      "1003\n",
      "4024\n",
      "3430\n",
      "1890\n",
      "2800\n",
      "2837\n",
      "6607\n",
      "686\n",
      "3319\n",
      "910\n",
      "1295\n",
      "3525\n",
      "4962\n",
      "4951\n",
      "1273\n",
      "7660\n",
      "5647\n",
      "2782\n",
      "1761\n",
      "3273\n",
      "7905\n",
      "2611\n",
      "7413\n",
      "2934\n",
      "6770\n",
      "4003\n",
      "2669\n",
      "2168\n",
      "7732\n",
      "3972\n",
      "3804\n",
      "2478\n",
      "2599\n",
      "1009\n",
      "5593\n",
      "9314\n",
      "1958\n",
      "8179\n",
      "5783\n",
      "4476\n",
      "4303\n",
      "2172\n",
      "8689\n",
      "1527\n",
      "9780\n",
      "6872\n",
      "5965\n",
      "4348\n",
      "728\n",
      "1694\n",
      "4134\n",
      "2208\n",
      "6387\n",
      "440\n",
      "3094\n",
      "3988\n",
      "2116\n",
      "2249\n",
      "923\n",
      "2219\n",
      "6504\n",
      "1428\n",
      "693\n",
      "7155\n",
      "3482\n",
      "4645\n",
      "1648\n",
      "2424\n",
      "3638\n",
      "5383\n",
      "1549\n",
      "9507\n",
      "2985\n",
      "6425\n",
      "5787\n",
      "3372\n",
      "866\n",
      "8620\n",
      "4291\n",
      "4163\n",
      "4392\n",
      "7067\n",
      "1380\n",
      "3539\n",
      "1929\n",
      "1211\n",
      "1104\n",
      "1325\n",
      "3185\n",
      "3198\n",
      "1301\n",
      "842\n",
      "3220\n",
      "7826\n",
      "7166\n",
      "2006\n",
      "2693\n",
      "6771\n",
      "6110\n",
      "5854\n",
      "7924\n",
      "9636\n",
      "3678\n",
      "4940\n",
      "9157\n",
      "5021\n",
      "5425\n",
      "1424\n",
      "3183\n",
      "3936\n",
      "3996\n",
      "4548\n",
      "1590\n",
      "6876\n",
      "3058\n"
     ]
    }
   ],
   "source": [
    "# get lengtsh of contexts\n",
    "for c in contexts:\n",
    "    print(len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'Since logistic regression corresponds to a linear neural network with no activation function, it should be apparent the multi-class neural network allows us to extend logistic regression to the multi-class setting  One way to accomplish this is to simply replace f(x, w) in eq  (15 18) with a linear function; while this is certainly a valid way to proceed, there is one slightly annoying side-effect',\n",
       " ' Recall from section 15 3 1 that in the case where we applied neural networks to a two-class classification task, the neural network had a single output neuron  However, in the multi-class setting considered in section 15 3',\n",
       " '2, the neural network had as many outputs C as there was classes  This means that this approach to multi-class regression would not directly generalize the binary classification case',\n",
       " ' To get around this, it is customary to implement linear multi-class classification using the (modified) softmax with C −1 inputs which we encountered in eq  (5 31)  Specifically, assume X˜ is our dataset transformed in the usual manner by pre-fixing it with 1, and X˜ has dimensions N × M']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sections vary great in size, so we need to split them into smaller chunks of at most size 100 chars\n",
    "\n",
    "# split sections into chunks of at most 100 chars\n",
    "def split_section(section, size=100):\n",
    "    section = section.split('.')\n",
    "    chunks = []\n",
    "    chunk = []\n",
    "    chunk_size = 0\n",
    "    for sentence in section:\n",
    "        chunk_size += len(sentence)\n",
    "        if chunk_size > size:\n",
    "            chunks.append(' '.join(chunk))\n",
    "            chunk = []\n",
    "            chunk_size = 0\n",
    "        chunk.append(sentence)\n",
    "    chunks.append(' '.join(chunk))\n",
    "    return chunks\n",
    "\n",
    "split_section(contexts[100], 200)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = [split_section(s, 200) for s in contexts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How can we build intelligent machines? More than 65 years ago Alan Turing made this question the subject of his famous essay “Computing machinery and intelligence” [Turing, 1950]',\n",
       " 'Alan Turing suggested that when we phrase the question in this manner, we unavoidably get bogged down in the definition of the word “intelligence”',\n",
       " 'Instead, he proposed we should rather consider a different question: Can we construct a machine that can do the same things a human can do? This may ultimately be as hard to answer as the first question, but at least we don’t have to begin our efforts by defining intelligence  A second part of Turing’s essay discuss how we might build such a human\\ufffeimitating machine',\n",
       " 'Turing proposed that instead of writing a computer program that behaves like a human from scratch, we should build a machine which initially cannot do a great many things but which can learn from past experience',\n",
       " 'For instance, if we wished to construct a machine which translate from English to French, we should instead construct a machine which is able to learn how to translate by observing examples of translated sentences in both languages, much like how a child acquires language']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flatten list of lists\n",
    "paragraphs = [item.strip() for sublist in paragraphs for item in sublist]\n",
    "paragraphs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2260"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for generation\n",
    "batch_size = 8 #Batch size\n",
    "num_queries = 5 #Number of queries to generate for every paragraph\n",
    "max_length_paragraph = 1000 #Max length for paragraph\n",
    "max_length_query = 200   #Max length for output query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Obviously, how to build these models will be a subject we return to several times in later chapters, however in this chapter we will be concerned with introducing a few building blocks which we will use many times over when constructing more elaborate models', 'Consider a setting where we consider a single, binary variable b which can be either false, b = 0, or true, b = 1', 'The prototypical example of a binary event is a coin flip where b = 0 denote the event the coin landed tails and b = 1 the event the coin landed heads, but the setup applies to all simple5', '4 The Bernoulli, categorical and binomial distributions 81 classification problems with two outcomes, for instance we could denote the event a treatment cures a patient such that b = 0 if the patient is not cured and b = 1 if the patient is cured  The Bernoulli distribution is the assumption the probability that b = 0 or b = 1 depends on a number 0 ≤ θ ≤ 1 as: Bernoulli distribution: p(b|θ) = θ b (1 − θ) 1−b', 'It is worth going over this in some detail  The left hand side says that given we know θ, then the probability of b (which has two outcomes) is the expression on the right', 'Notice that: p(b = 0|θ) = θ 0 (1 − θ) 1−0 = 1 − θ p(b = 1|θ) = θ 1 (1 − θ) 1−1 = θ and therefore, we can interpret θ as simply being the probability b = 1  As an example, we can compute the mean and variance of the Bernoulli distribution: E[b] = X 1 b=0 bp(b|θ) = 0 × (1 − θ) + 1 × θ = θ, (5 27a) Var[b] = (0 − θ) 2 × (1 − θ) + (1 − θ) 2 × θ = θ(1 − θ)  (5', '27b) A notational problem we can anticipate is that when we use p to denote all sorts of probability densities, it can become difficult to figure out exactly which we refer to  It is therefore common to introduce special notation for familiar densities  We will therefore sometimes write: Bernouilli(b|θ) = p(b|θ) to signify the Bernoulli distribution', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:09<00:00, 69.11s/it]\n"
     ]
    }
   ],
   "source": [
    "# Now for every paragraph in our corpus, we generate the queries\n",
    "start = 500\n",
    "with open('generated_queries_jason.tsv', 'w') as fOut:\n",
    "    for start_idx in tqdm.trange(start, start+batch_size, batch_size):\n",
    "        sub_paragraphs = paragraphs[start_idx:start_idx+batch_size]\n",
    "        print(sub_paragraphs)\n",
    "        inputs = tokenizer.prepare_seq2seq_batch(sub_paragraphs, max_length=max_length_paragraph, truncation=True, return_tensors='pt').to(device)\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length_query,\n",
    "            do_sample=True,\n",
    "            top_p=0.95,\n",
    "            num_return_sequences=num_queries)\n",
    "\n",
    "        for idx, out in enumerate(outputs):\n",
    "            query = tokenizer.decode(out, skip_special_tokens=True)\n",
    "            para = sub_paragraphs[int(idx/num_queries)]\n",
    "            fOut.write(\"{}\\t{}\\n\".format(query.replace(\"\\t\", \" \").strip(), para.replace(\"\\t\", \" \").strip()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
