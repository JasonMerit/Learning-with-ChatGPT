{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UKPLab / sentence-transformers\n",
    "https://github.com/UKPLab/sentence-transformers/tree/master/examples/unsupervised_learning/query_generation\n",
    "\n",
    "- 1_programming_query_generation.py - We generate queries for all paragraphs from these articles \n",
    "- 2_programming_train_bi-encoder.py - We train a SentenceTransformer bi-encoder with these generated queries. This results in a model we can then use for sematic search (for the given Wikipedia articles).\n",
    "- 3_programming_semantic_search.py - Shows how the trained model can be used for semantic search\n",
    "\n",
    "## 1_programming_query_generation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "import tqdm\n",
    "import os\n",
    "from sentence_transformers import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sections pickle\n",
    "import pickle\n",
    "# with open('df_context.pkl', 'rb') as f:\n",
    "with open('../al_experiment\\df_pickle\\df_context.pkl', 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "\n",
    "contexts = df['context'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "437.57246376811594"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# contexts[:5]\n",
    "# [len(p) for p in paragraphs[:10]]\n",
    "# sum([len(p) for p in paragraphs]) / len(paragraphs)\n",
    "# len(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sections vary great in size, so we need to split them into smaller chunks of at most size 100 chars\n",
    "\n",
    "# split sections into chunks of at most 'size' chars\n",
    "def split_paragraph(context, size=100):\n",
    "    sentences = [(e + '.').strip() for e in context.split('. ') if e]  # important with space after dot!\n",
    "    chunks, chunk, chunk_size = [], [], 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        chunk_size += len(sentence)\n",
    "        if chunk_size > size:\n",
    "            chunks.append(' '.join(chunk))\n",
    "            chunk, chunk_size = [], 0\n",
    "        chunk.append(sentence)\n",
    "\n",
    "    chunks.append(' '.join(chunk)) # add last chunk\n",
    "    return chunks\n",
    "\n",
    "# split_paragraph(contexts[0], 376)[:5]\n",
    "paragraphs = [split_paragraph(c, 376) for c in contexts]\n",
    "paragraphs = [p for sublist in paragraphs for p in sublist] # flatten list\n",
    "paragraphs = [p for p in paragraphs if len(p) > 10] # remove empty paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we load the model that is able to generate queries given a paragraph.\n",
    "# This model was trained on the MS MARCO dataset, a dataset with 500k\n",
    "# queries from Bing and the respective relevant passage\n",
    "tokenizer = T5Tokenizer.from_pretrained('BeIR/query-gen-msmarco-t5-large-v1')\n",
    "model = T5ForConditionalGeneration.from_pretrained('BeIR/query-gen-msmarco-t5-large-v1')\n",
    "model.eval()\n",
    "\n",
    "#Select the device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "# Parameters for generation\n",
    "batch_size = 8 #Batch size\n",
    "num_queries = 5 #Number of queries to generate for every paragraph\n",
    "max_length_paragraph = 300 #Max length for paragraph\n",
    "max_length_query = 64   #Max length for output query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _start_idx = 120\n",
    "# _sub_paragraphs = paragraphs[_start_idx:_start_idx+batch_size]\n",
    "# # kek = tokenizer.prepare_seq2seq_batch(_sub_paragraphs, max_length=max_length_paragraph, truncation=True, return_tensors='pt').to(device)\n",
    "# lol = tokenizer(src_texts=_sub_paragraphs, max_length=max_length_paragraph, truncation=True, return_tensors='pt').to(device)\n",
    "# lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/173 [00:00<?, ?it/s]c:\\Users\\Jason\\miniconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3712: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)\n",
      "\n",
      "If you either need to use different keyword arguments for the source and target texts, you should do two calls like\n",
      "this:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "labels = tokenizer(text_target=tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n",
      "100%|██████████| 173/173 [4:47:12<00:00, 99.61s/it]   \n"
     ]
    }
   ],
   "source": [
    "# Now for every paragraph in our corpus, we generate the queries\n",
    "with open('jason_generated_queries.tsv', 'w') as fOut:\n",
    "    # for start_idx in tqdm.trange(0, 10*batch_size, batch_size):\n",
    "    for start_idx in tqdm.trange(0, len(paragraphs), batch_size):\n",
    "        sub_paragraphs = paragraphs[start_idx:start_idx+batch_size]\n",
    "        inputs = tokenizer.prepare_seq2seq_batch(sub_paragraphs, max_length=max_length_paragraph, truncation=True, return_tensors='pt').to(device)\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length_query,\n",
    "            do_sample=True,\n",
    "            top_p=0.95,\n",
    "            num_return_sequences=num_queries)\n",
    "\n",
    "        for idx, out in enumerate(outputs):\n",
    "            query = tokenizer.decode(out, skip_special_tokens=True)\n",
    "            para = sub_paragraphs[int(idx/num_queries)]\n",
    "            fOut.write(\"{}\\t{}\\n\".format(query.replace(\"\\t\", \" \").strip(), para.replace(\"\\t\", \" \").strip()))\n",
    "# 287m"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2_programming_train_bi-encoder.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses, models, datasets\n",
    "import os\n",
    "\n",
    "train_examples = []\n",
    "with open('jason_generated_queries.tsv') as fIn:\n",
    "    for line in fIn:\n",
    "        try:\n",
    "            query, paragraph = line.strip().split('\\t', maxsplit=1)\n",
    "            train_examples.append(InputExample(texts=[query, paragraph]))\n",
    "        except ValueError:\n",
    "            print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# For the MultipleNegativesRankingLoss, it is important\n",
    "# that the batch does not contain duplicate entries, i.e.\n",
    "# no two equal queries and no two equal paragraphs.\n",
    "# To ensure this, we use a special data loader\n",
    "train_dataloader = datasets.NoDuplicatesDataLoader(train_examples, batch_size=64)\n",
    "\n",
    "# Now we create a SentenceTransformer model from scratch\n",
    "word_emb = models.Transformer('distilbert-base-uncased')\n",
    "pooling = models.Pooling(word_emb.get_word_embedding_dimension())\n",
    "model = SentenceTransformer(modules=[word_emb, pooling])\n",
    "\n",
    "# MultipleNegativesRankingLoss requires input pairs (query, relevant_passage)\n",
    "# and trains the model so that is is suitable for semantic search\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6b0dc291a234f0490bcf2f49064224f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bbb10df3cb44f54b259352ad3d2fa46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/107 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 302886912 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n\u001b[0;32m      3\u001b[0m warmup_steps \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39mlen\u001b[39m(train_dataloader) \u001b[39m*\u001b[39m num_epochs \u001b[39m*\u001b[39m \u001b[39m0.1\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m model\u001b[39m.\u001b[39;49mfit(train_objectives\u001b[39m=\u001b[39;49m[(train_dataloader, train_loss)], epochs\u001b[39m=\u001b[39;49mnum_epochs, warmup_steps\u001b[39m=\u001b[39;49mwarmup_steps, show_progress_bar\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      6\u001b[0m os\u001b[39m.\u001b[39mmakedirs(\u001b[39m'\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m'\u001b[39m, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m model\u001b[39m.\u001b[39msave(\u001b[39m'\u001b[39m\u001b[39moutput/machine_learning-model\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Jason\\miniconda3\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:721\u001b[0m, in \u001b[0;36mSentenceTransformer.fit\u001b[1;34m(self, train_objectives, evaluator, epochs, steps_per_epoch, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, evaluation_steps, output_path, save_best_model, max_grad_norm, use_amp, callback, show_progress_bar, checkpoint_path, checkpoint_save_steps, checkpoint_save_total_limit)\u001b[0m\n\u001b[0;32m    719\u001b[0m     skip_scheduler \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mget_scale() \u001b[39m!=\u001b[39m scale_before_step\n\u001b[0;32m    720\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 721\u001b[0m     loss_value \u001b[39m=\u001b[39m loss_model(features, labels)\n\u001b[0;32m    722\u001b[0m     loss_value\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m    723\u001b[0m     torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(loss_model\u001b[39m.\u001b[39mparameters(), max_grad_norm)\n",
      "File \u001b[1;32mc:\\Users\\Jason\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Jason\\miniconda3\\lib\\site-packages\\sentence_transformers\\losses\\MultipleNegativesRankingLoss.py:53\u001b[0m, in \u001b[0;36mMultipleNegativesRankingLoss.forward\u001b[1;34m(self, sentence_features, labels)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, sentence_features: Iterable[Dict[\u001b[39mstr\u001b[39m, Tensor]], labels: Tensor):\n\u001b[1;32m---> 53\u001b[0m     reps \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(sentence_feature)[\u001b[39m'\u001b[39m\u001b[39msentence_embedding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m sentence_feature \u001b[39min\u001b[39;00m sentence_features]\n\u001b[0;32m     54\u001b[0m     embeddings_a \u001b[39m=\u001b[39m reps[\u001b[39m0\u001b[39m]\n\u001b[0;32m     55\u001b[0m     embeddings_b \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(reps[\u001b[39m1\u001b[39m:])\n",
      "File \u001b[1;32mc:\\Users\\Jason\\miniconda3\\lib\\site-packages\\sentence_transformers\\losses\\MultipleNegativesRankingLoss.py:53\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, sentence_features: Iterable[Dict[\u001b[39mstr\u001b[39m, Tensor]], labels: Tensor):\n\u001b[1;32m---> 53\u001b[0m     reps \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(sentence_feature)[\u001b[39m'\u001b[39m\u001b[39msentence_embedding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m sentence_feature \u001b[39min\u001b[39;00m sentence_features]\n\u001b[0;32m     54\u001b[0m     embeddings_a \u001b[39m=\u001b[39m reps[\u001b[39m0\u001b[39m]\n\u001b[0;32m     55\u001b[0m     embeddings_b \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(reps[\u001b[39m1\u001b[39m:])\n",
      "File \u001b[1;32mc:\\Users\\Jason\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Jason\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Jason\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Jason\\miniconda3\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py:66\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m features:\n\u001b[0;32m     64\u001b[0m     trans_features[\u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m features[\u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m---> 66\u001b[0m output_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrans_features, return_dict\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     67\u001b[0m output_tokens \u001b[39m=\u001b[39m output_states[\u001b[39m0\u001b[39m]\n\u001b[0;32m     69\u001b[0m features\u001b[39m.\u001b[39mupdate({\u001b[39m'\u001b[39m\u001b[39mtoken_embeddings\u001b[39m\u001b[39m'\u001b[39m: output_tokens, \u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m: features[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m]})\n",
      "File \u001b[1;32mc:\\Users\\Jason\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Jason\\miniconda3\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:583\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    579\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    581\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(input_ids, inputs_embeds)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[0;32m    584\u001b[0m     x\u001b[39m=\u001b[39;49membeddings,\n\u001b[0;32m    585\u001b[0m     attn_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    586\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    587\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    588\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    589\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    590\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Jason\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Jason\\miniconda3\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:359\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[39mif\u001b[39;00m output_hidden_states:\n\u001b[0;32m    357\u001b[0m     all_hidden_states \u001b[39m=\u001b[39m all_hidden_states \u001b[39m+\u001b[39m (hidden_state,)\n\u001b[1;32m--> 359\u001b[0m layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    360\u001b[0m     x\u001b[39m=\u001b[39;49mhidden_state, attn_mask\u001b[39m=\u001b[39;49mattn_mask, head_mask\u001b[39m=\u001b[39;49mhead_mask[i], output_attentions\u001b[39m=\u001b[39;49moutput_attentions\n\u001b[0;32m    361\u001b[0m )\n\u001b[0;32m    362\u001b[0m hidden_state \u001b[39m=\u001b[39m layer_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    364\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\Jason\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Jason\\miniconda3\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:295\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    286\u001b[0m \u001b[39mParameters:\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[39m    x: torch.tensor(bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[39m    torch.tensor(bs, seq_length, dim) The output of the transformer block contextualization.\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[39m# Self-Attention\u001b[39;00m\n\u001b[1;32m--> 295\u001b[0m sa_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[0;32m    296\u001b[0m     query\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m    297\u001b[0m     key\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m    298\u001b[0m     value\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m    299\u001b[0m     mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[0;32m    300\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    301\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    302\u001b[0m )\n\u001b[0;32m    303\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[0;32m    304\u001b[0m     sa_output, sa_weights \u001b[39m=\u001b[39m sa_output  \u001b[39m# (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jason\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Jason\\miniconda3\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:220\u001b[0m, in \u001b[0;36mMultiHeadSelfAttention.forward\u001b[1;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    217\u001b[0m v \u001b[39m=\u001b[39m shape(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_lin(value))  \u001b[39m# (bs, n_heads, k_length, dim_per_head)\u001b[39;00m\n\u001b[0;32m    219\u001b[0m q \u001b[39m=\u001b[39m q \u001b[39m/\u001b[39m math\u001b[39m.\u001b[39msqrt(dim_per_head)  \u001b[39m# (bs, n_heads, q_length, dim_per_head)\u001b[39;00m\n\u001b[1;32m--> 220\u001b[0m scores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(q, k\u001b[39m.\u001b[39;49mtranspose(\u001b[39m2\u001b[39;49m, \u001b[39m3\u001b[39;49m))  \u001b[39m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n\u001b[0;32m    221\u001b[0m mask \u001b[39m=\u001b[39m (mask \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mview(mask_reshp)\u001b[39m.\u001b[39mexpand_as(scores)  \u001b[39m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n\u001b[0;32m    222\u001b[0m scores \u001b[39m=\u001b[39m scores\u001b[39m.\u001b[39mmasked_fill(\n\u001b[0;32m    223\u001b[0m     mask, torch\u001b[39m.\u001b[39mtensor(torch\u001b[39m.\u001b[39mfinfo(scores\u001b[39m.\u001b[39mdtype)\u001b[39m.\u001b[39mmin)\n\u001b[0;32m    224\u001b[0m )  \u001b[39m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 302886912 bytes."
     ]
    }
   ],
   "source": [
    "# Tune the model\n",
    "num_epochs = 3\n",
    "warmup_steps = int(len(train_dataloader) * num_epochs * 0.1)\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=num_epochs, warmup_steps=warmup_steps, show_progress_bar=True)\n",
    "\n",
    "os.makedirs('output', exist_ok=True)\n",
    "model.save('output/machine_learning-model')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3_programming_semantic_search.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import gzip\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Load the model we trained in 2_programming_train_bi-encoder.py\n",
    "model = SentenceTransformer('output/programming-model')\n",
    "\n",
    "# Load the corpus\n",
    "docs = []\n",
    "corpus_filepath = 'wiki-programmming-20210101.jsonl.gz'\n",
    "if not os.path.exists(corpus_filepath):\n",
    "    util.http_get('https://sbert.net/datasets/wiki-programmming-20210101.jsonl.gz', corpus_filepath)\n",
    "\n",
    "with gzip.open(corpus_filepath, 'rt') as fIn:\n",
    "    for line in fIn:\n",
    "        data = json.loads(line.strip())\n",
    "        title = data['title']\n",
    "        for p in data['paragraphs']:\n",
    "            if len(p) > 100:    #Only take paragraphs with at least 100 chars\n",
    "                docs.append((title, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_emb = model.encode([d[1] for d in docs], convert_to_tensor=True)  # TODO: Should be done once\n",
    "# Save paragraph embeddings to disk\n",
    "with open('output/ML-paragraph-embeddings.pkl', 'wb') as fOut:\n",
    "    pickle.dump(paragraph_emb, fOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"Available Wikipedia Articles:\")\n",
    "print(\", \".join(sorted(list(set([d[0] for d in docs])))))\n",
    "\n",
    "# Example for semantic search\n",
    "# while True:\n",
    "#     query = input(\"Query: \")\n",
    "#     query_emb = model.encode(query, convert_to_tensor=True)\n",
    "#     hits = util.semantic_search(query_emb, paragraph_emb, top_k=3)[0]\n",
    "\n",
    "#     for hit in hits:\n",
    "#         doc = docs[hit['corpus_id']]\n",
    "#         print(\"{:.2f}\\t{}\\t\\t{}\".format(hit['score'], doc[0], doc[1]))\n",
    "\n",
    "#     print(\"\\n=================\\n\")\n",
    "\n",
    "query = \"How to sort a list in Python?\"\n",
    "query_emb = model.encode(query, convert_to_tensor=True)\n",
    "hits = util.semantic_search(query_emb, paragraph_emb, top_k=3)[0]\n",
    "\n",
    "for hit in hits:\n",
    "    doc = docs[hit['corpus_id']]\n",
    "    print(\"{:.2f}\\t{}\\t\\t{}\".format(hit['score'], doc[0], doc[1]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sections pickle\n",
    "import pickle\n",
    "# add df_pickle to path\n",
    "with open('df_context.pkl', 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "\n",
    "# Extract context\n",
    "contexts = df['context'].tolist()\n",
    "contexts = [c.strip() for c in contexts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1183\n",
      "691\n",
      "838\n",
      "415\n",
      "1333\n",
      "3658\n",
      "296\n",
      "4139\n",
      "7414\n",
      "3579\n",
      "2250\n",
      "1411\n",
      "7674\n",
      "2930\n",
      "3979\n",
      "1654\n",
      "3990\n",
      "2898\n",
      "1766\n",
      "821\n",
      "2872\n",
      "2991\n",
      "9969\n",
      "1550\n",
      "1107\n",
      "2769\n",
      "5361\n",
      "3734\n",
      "4172\n",
      "1300\n",
      "10140\n",
      "1594\n",
      "1726\n",
      "3380\n",
      "3675\n",
      "2775\n",
      "8407\n",
      "7129\n",
      "3008\n",
      "3124\n",
      "1600\n",
      "1132\n",
      "1362\n",
      "4432\n",
      "1160\n",
      "1003\n",
      "4024\n",
      "3430\n",
      "1890\n",
      "2800\n",
      "2837\n",
      "6607\n",
      "686\n",
      "3319\n",
      "910\n",
      "1295\n",
      "3525\n",
      "4962\n",
      "4951\n",
      "1273\n",
      "7660\n",
      "5647\n",
      "2782\n",
      "1761\n",
      "3273\n",
      "7905\n",
      "2611\n",
      "7413\n",
      "2934\n",
      "6770\n",
      "4003\n",
      "2669\n",
      "2168\n",
      "7732\n",
      "3972\n",
      "3804\n",
      "2478\n",
      "2599\n",
      "1009\n",
      "5593\n",
      "9314\n",
      "1958\n",
      "8179\n",
      "5783\n",
      "4476\n",
      "4303\n",
      "2172\n",
      "8689\n",
      "1527\n",
      "9780\n",
      "6872\n",
      "5965\n",
      "4348\n",
      "728\n",
      "1694\n",
      "4134\n",
      "2208\n",
      "6387\n",
      "440\n",
      "3094\n",
      "3988\n",
      "2116\n",
      "2249\n",
      "923\n",
      "2219\n",
      "6504\n",
      "1428\n",
      "693\n",
      "7155\n",
      "3482\n",
      "4645\n",
      "1648\n",
      "2424\n",
      "3638\n",
      "5383\n",
      "1549\n",
      "9507\n",
      "2985\n",
      "6425\n",
      "5787\n",
      "3372\n",
      "866\n",
      "8620\n",
      "4291\n",
      "4163\n",
      "4392\n",
      "7067\n",
      "1380\n",
      "3539\n",
      "1929\n",
      "1211\n",
      "1104\n",
      "1325\n",
      "3185\n",
      "3198\n",
      "1301\n",
      "842\n",
      "3220\n",
      "7826\n",
      "7166\n",
      "2006\n",
      "2693\n",
      "6771\n",
      "6110\n",
      "5854\n",
      "7924\n",
      "9636\n",
      "3678\n",
      "4940\n",
      "9157\n",
      "5021\n",
      "5425\n",
      "1424\n",
      "3183\n",
      "3936\n",
      "3996\n",
      "4548\n",
      "1590\n",
      "6876\n",
      "3058\n"
     ]
    }
   ],
   "source": [
    "# get lengtsh of contexts\n",
    "for c in contexts:\n",
    "    print(len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'Since logistic regression corresponds to a linear neural network with no activation function, it should be apparent the multi-class neural network allows us to extend logistic regression to the multi-class setting  One way to accomplish this is to simply replace f(x, w) in eq  (15 18) with a linear function; while this is certainly a valid way to proceed, there is one slightly annoying side-effect',\n",
       " ' Recall from section 15 3 1 that in the case where we applied neural networks to a two-class classification task, the neural network had a single output neuron  However, in the multi-class setting considered in section 15 3',\n",
       " '2, the neural network had as many outputs C as there was classes  This means that this approach to multi-class regression would not directly generalize the binary classification case',\n",
       " ' To get around this, it is customary to implement linear multi-class classification using the (modified) softmax with C −1 inputs which we encountered in eq  (5 31)  Specifically, assume X˜ is our dataset transformed in the usual manner by pre-fixing it with 1, and X˜ has dimensions N × M']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sections vary great in size, so we need to split them into smaller chunks of at most size 100 chars\n",
    "\n",
    "# split sections into chunks of at most 100 chars\n",
    "def split_section(section, size=100):\n",
    "    section = section.split('.')\n",
    "    chunks = []\n",
    "    chunk = []\n",
    "    chunk_size = 0\n",
    "    for sentence in section:\n",
    "        chunk_size += len(sentence)\n",
    "        if chunk_size > size:\n",
    "            chunks.append(' '.join(chunk))\n",
    "            chunk = []\n",
    "            chunk_size = 0\n",
    "        chunk.append(sentence)\n",
    "    chunks.append(' '.join(chunk))\n",
    "    return chunks\n",
    "\n",
    "split_section(contexts[100], 200)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = [split_section(s, 200) for s in contexts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How can we build intelligent machines? More than 65 years ago Alan Turing made this question the subject of his famous essay “Computing machinery and intelligence” [Turing, 1950]',\n",
       " 'Alan Turing suggested that when we phrase the question in this manner, we unavoidably get bogged down in the definition of the word “intelligence”',\n",
       " 'Instead, he proposed we should rather consider a different question: Can we construct a machine that can do the same things a human can do? This may ultimately be as hard to answer as the first question, but at least we don’t have to begin our efforts by defining intelligence  A second part of Turing’s essay discuss how we might build such a human\\ufffeimitating machine',\n",
       " 'Turing proposed that instead of writing a computer program that behaves like a human from scratch, we should build a machine which initially cannot do a great many things but which can learn from past experience',\n",
       " 'For instance, if we wished to construct a machine which translate from English to French, we should instead construct a machine which is able to learn how to translate by observing examples of translated sentences in both languages, much like how a child acquires language']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flatten list of lists\n",
    "paragraphs = [item.strip() for sublist in paragraphs for item in sublist]\n",
    "paragraphs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2260"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for generation\n",
    "batch_size = 8 #Batch size\n",
    "num_queries = 5 #Number of queries to generate for every paragraph\n",
    "max_length_paragraph = 1000 #Max length for paragraph\n",
    "max_length_query = 200   #Max length for output query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Obviously, how to build these models will be a subject we return to several times in later chapters, however in this chapter we will be concerned with introducing a few building blocks which we will use many times over when constructing more elaborate models', 'Consider a setting where we consider a single, binary variable b which can be either false, b = 0, or true, b = 1', 'The prototypical example of a binary event is a coin flip where b = 0 denote the event the coin landed tails and b = 1 the event the coin landed heads, but the setup applies to all simple5', '4 The Bernoulli, categorical and binomial distributions 81 classification problems with two outcomes, for instance we could denote the event a treatment cures a patient such that b = 0 if the patient is not cured and b = 1 if the patient is cured  The Bernoulli distribution is the assumption the probability that b = 0 or b = 1 depends on a number 0 ≤ θ ≤ 1 as: Bernoulli distribution: p(b|θ) = θ b (1 − θ) 1−b', 'It is worth going over this in some detail  The left hand side says that given we know θ, then the probability of b (which has two outcomes) is the expression on the right', 'Notice that: p(b = 0|θ) = θ 0 (1 − θ) 1−0 = 1 − θ p(b = 1|θ) = θ 1 (1 − θ) 1−1 = θ and therefore, we can interpret θ as simply being the probability b = 1  As an example, we can compute the mean and variance of the Bernoulli distribution: E[b] = X 1 b=0 bp(b|θ) = 0 × (1 − θ) + 1 × θ = θ, (5 27a) Var[b] = (0 − θ) 2 × (1 − θ) + (1 − θ) 2 × θ = θ(1 − θ)  (5', '27b) A notational problem we can anticipate is that when we use p to denote all sorts of probability densities, it can become difficult to figure out exactly which we refer to  It is therefore common to introduce special notation for familiar densities  We will therefore sometimes write: Bernouilli(b|θ) = p(b|θ) to signify the Bernoulli distribution', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:09<00:00, 69.11s/it]\n"
     ]
    }
   ],
   "source": [
    "# Now for every paragraph in our corpus, we generate the queries\n",
    "start = 500\n",
    "with open('generated_queries_jason.tsv', 'w') as fOut:\n",
    "    for start_idx in tqdm.trange(start, start+batch_size, batch_size):\n",
    "        sub_paragraphs = paragraphs[start_idx:start_idx+batch_size]\n",
    "        print(sub_paragraphs)\n",
    "        inputs = tokenizer.prepare_seq2seq_batch(sub_paragraphs, max_length=max_length_paragraph, truncation=True, return_tensors='pt').to(device)\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length_query,\n",
    "            do_sample=True,\n",
    "            top_p=0.95,\n",
    "            num_return_sequences=num_queries)\n",
    "\n",
    "        for idx, out in enumerate(outputs):\n",
    "            query = tokenizer.decode(out, skip_special_tokens=True)\n",
    "            para = sub_paragraphs[int(idx/num_queries)]\n",
    "            fOut.write(\"{}\\t{}\\n\".format(query.replace(\"\\t\", \" \").strip(), para.replace(\"\\t\", \" \").strip()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
